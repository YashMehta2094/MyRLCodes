{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYl9HElrTEim",
        "outputId": "18331115-3e1a-400e-ff3c-cf4413c35cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "pip install gym\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vPnFzWLTLS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c33042-a58d-4628-8b17-a7f5beda1a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q swig\n",
        "!pip install -q gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JBm8bXFiGe_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "class capture_video():\n",
        "    def __init__(self,video_lim=4,dim=(600,600)):\n",
        "\n",
        "        self.video_num = {}\n",
        "        self.images = [] # images will be stored here\n",
        "\n",
        "        self.backup_limit = video_lim # we will backup upto no of video given in video limit\n",
        "        self.dim = dim # dim of the stored video\n",
        "\n",
        "    def store_video(self,fps=30,model_name='PPO1'):\n",
        "\n",
        "        self.model_name = model_name\n",
        "        if model_name not in self.video_num.keys(): self.video_num[model_name] = 0 # creating the key to store no of videos stored in specific name\n",
        "\n",
        "        if not os.path.exists(f'./video/{self.model_name}/'): os.mkdir(f'./video/{self.model_name}/') # creating a dir if the dir doesn't exists\n",
        "\n",
        "        video_converter =  cv.VideoWriter_fourcc(*'mp4v') # creating video_writer\n",
        "        video = cv.VideoWriter(f'./video/{self.model_name}/video-{self.video_num[model_name]}.avi',video_converter,fps,self.dim)\n",
        "\n",
        "\n",
        "        if len(self.images) == 0 : print(f\" can't store videos as no images has been stored\")\n",
        "        else: # storing the video\n",
        "            [video.write(np.array(image)) for image in self.images]\n",
        "            video.release()\n",
        "            print(f' video n=in folder {model_name} and number {self.video_num[model_name]} has been stored')\n",
        "            self.video_num[model_name] = (self.video_num[model_name] % 4) + 1\n",
        "\n",
        "    def clear_images(self): # erasing the images\n",
        "        self.images = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.video_num = {}\n",
        "        self.images = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRzaDr41TFY9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        policy = self.actor(x)\n",
        "        value = self.critic(x)\n",
        "        return policy, value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWlx-nHPTJPz",
        "outputId": "7c2b0607-7700-4dd1-bb12-e2e3b127b7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Loss: 445.5770263671875, Rewards = -183.17197112170751\n",
            "Episode 10, Loss: 454.6297607421875, Rewards = -132.56917525154938\n",
            "Episode 20, Loss: 562.0738525390625, Rewards = -65.39091641964544\n",
            "Episode 30, Loss: 402.86053466796875, Rewards = -119.1914665885368\n",
            "Episode 40, Loss: 322.295654296875, Rewards = -88.08740283490634\n",
            "Episode 50, Loss: 307.3476867675781, Rewards = -102.21511549103609\n",
            "Episode 60, Loss: 384.0231628417969, Rewards = -90.30188716861196\n",
            "Episode 70, Loss: 1112.3878173828125, Rewards = -316.4326684118374\n",
            "Episode 80, Loss: 275.2765808105469, Rewards = 40.89660670941653\n",
            "Episode 90, Loss: 296.0479431152344, Rewards = -199.16278771784863\n",
            "Episode 100, Loss: 1199.486572265625, Rewards = -350.7816162063899\n",
            "Episode 110, Loss: 668.658203125, Rewards = -46.1979379570313\n",
            "Episode 120, Loss: 331.2369384765625, Rewards = -101.83241285041444\n",
            "Episode 130, Loss: 657.548583984375, Rewards = -269.80027313582195\n",
            "Episode 140, Loss: 415.19158935546875, Rewards = -354.17901640725427\n",
            "Episode 150, Loss: 578.3926391601562, Rewards = -54.49973750168182\n",
            "Episode 160, Loss: 544.5658569335938, Rewards = -178.63033162881584\n",
            "Episode 170, Loss: 428.7412414550781, Rewards = -239.21870826486105\n",
            "Episode 180, Loss: 416.46173095703125, Rewards = -114.0332631573142\n",
            "Episode 190, Loss: 480.0458984375, Rewards = -116.79051203460114\n",
            "Episode 200, Loss: 527.0902099609375, Rewards = -216.4462177851747\n",
            "Episode 210, Loss: 287.11785888671875, Rewards = -309.675710768316\n",
            "Episode 220, Loss: 236.54275512695312, Rewards = -233.5014064059146\n",
            "Episode 230, Loss: 449.94122314453125, Rewards = -89.99944652990861\n",
            "Episode 240, Loss: 783.3558349609375, Rewards = -167.81576023014406\n",
            "Episode 250, Loss: 375.4307861328125, Rewards = -258.1976023600623\n",
            "Episode 260, Loss: 482.52252197265625, Rewards = -161.68137404332452\n",
            "Episode 270, Loss: 387.7587585449219, Rewards = -75.68479002900162\n",
            "Episode 280, Loss: 179.10569763183594, Rewards = -340.52922728912415\n",
            "Episode 290, Loss: 348.4459533691406, Rewards = -55.281889192839614\n",
            "Episode 300, Loss: 194.62881469726562, Rewards = -106.07487513862797\n",
            "Episode 310, Loss: 674.788818359375, Rewards = -469.24552348638167\n",
            "Episode 320, Loss: 181.772705078125, Rewards = -190.86383901744182\n",
            "Episode 330, Loss: 873.7182006835938, Rewards = -332.57790887317736\n",
            "Episode 340, Loss: 336.2154235839844, Rewards = -91.94566142846762\n",
            "Episode 350, Loss: 280.1114501953125, Rewards = -114.00555290259746\n",
            "Episode 360, Loss: 722.6884155273438, Rewards = -266.38887340721107\n",
            "Episode 370, Loss: 377.2071228027344, Rewards = -17.122545677158087\n",
            "Episode 380, Loss: 728.2976684570312, Rewards = -320.8777409462866\n",
            "Episode 390, Loss: 258.7310485839844, Rewards = -49.8300190856441\n",
            "Episode 400, Loss: 716.3656005859375, Rewards = -256.34833075166375\n",
            "Episode 410, Loss: 2207.476806640625, Rewards = -304.97501623127647\n",
            "Episode 420, Loss: 2796.309814453125, Rewards = -527.4149389641022\n",
            "Episode 430, Loss: 242.63902282714844, Rewards = -42.63767158175492\n",
            "Episode 440, Loss: 1055.84423828125, Rewards = -177.67826511822733\n",
            "Episode 450, Loss: 334.97930908203125, Rewards = 8.694894872370753\n",
            "Episode 460, Loss: 528.8312377929688, Rewards = -241.79706042654772\n",
            "Episode 470, Loss: 449.8373107910156, Rewards = -184.11091963413264\n",
            "Episode 480, Loss: 187.39767456054688, Rewards = -140.0201808581038\n",
            "Episode 490, Loss: 518.3129272460938, Rewards = -165.93597646353925\n",
            "Episode 500, Loss: 266.0704650878906, Rewards = -211.6226414219124\n",
            "Episode 510, Loss: 594.6740112304688, Rewards = -213.95527421835186\n",
            "Episode 520, Loss: 171.1715087890625, Rewards = -105.96588211763988\n",
            "Episode 530, Loss: 215.38966369628906, Rewards = -20.20872118650064\n",
            "Episode 540, Loss: 92.2378158569336, Rewards = -30.19511192305643\n",
            "Episode 550, Loss: 348.0904235839844, Rewards = 4.4743514338644275\n",
            "Episode 560, Loss: 319.7972412109375, Rewards = -36.70302305186675\n",
            "Episode 570, Loss: 229.20033264160156, Rewards = -66.04731560829381\n",
            "Episode 580, Loss: 160.54078674316406, Rewards = -218.9426395291428\n",
            "Episode 590, Loss: 265.8982849121094, Rewards = 19.56836528359895\n",
            "Episode 600, Loss: 139.70982360839844, Rewards = -115.64423551110633\n",
            "Episode 610, Loss: 255.50314331054688, Rewards = 8.39854703568703\n",
            "Episode 620, Loss: 217.67408752441406, Rewards = -127.1237732190501\n",
            "Episode 630, Loss: 452.62847900390625, Rewards = -180.33705256513497\n",
            "Episode 640, Loss: 156.98069763183594, Rewards = -69.05541796253151\n",
            "Episode 650, Loss: 131.3399658203125, Rewards = -60.83940720626995\n",
            "Episode 660, Loss: 127.17564392089844, Rewards = -107.24577615472991\n",
            "Episode 670, Loss: 91.88990783691406, Rewards = -5.6987421577123545\n",
            "Episode 680, Loss: 73.5599136352539, Rewards = 13.709577892165667\n",
            "Episode 690, Loss: 79.70158386230469, Rewards = 13.32103567080811\n",
            "Episode 700, Loss: 108.40153503417969, Rewards = -86.91055242509708\n",
            "Episode 710, Loss: 134.07977294921875, Rewards = -196.985794453452\n",
            "Episode 720, Loss: 34.52272415161133, Rewards = -248.3876532549856\n",
            "Episode 730, Loss: 87.49752044677734, Rewards = 31.564521987701642\n",
            "Episode 740, Loss: 126.5233383178711, Rewards = -108.0783385394612\n",
            "Episode 750, Loss: 124.47203063964844, Rewards = 3.714999131281843\n",
            "Episode 760, Loss: 113.92434692382812, Rewards = -30.672346596483013\n",
            "Episode 770, Loss: 114.30563354492188, Rewards = -32.95422129733282\n",
            "Episode 780, Loss: 115.08479309082031, Rewards = -90.30878119344779\n",
            "Episode 790, Loss: 64.73139190673828, Rewards = 28.86809748994493\n",
            "Episode 800, Loss: 65.9193344116211, Rewards = 63.026197393158554\n",
            "Episode 810, Loss: 35.51408004760742, Rewards = -14.559395463529183\n",
            "Episode 820, Loss: 29.58746910095215, Rewards = -72.03378661070344\n",
            "Episode 830, Loss: 86.02908325195312, Rewards = 19.055265817856938\n",
            "Episode 840, Loss: 47.89350128173828, Rewards = -54.532758334114824\n",
            "Episode 850, Loss: 44.34879684448242, Rewards = 13.37418664931577\n",
            "Episode 860, Loss: 21.511821746826172, Rewards = -107.06342124252755\n",
            "Episode 870, Loss: 28.662368774414062, Rewards = -59.48260543057101\n",
            "Episode 880, Loss: 42.79848098754883, Rewards = 9.035855965036133\n",
            "Episode 890, Loss: 179.80661010742188, Rewards = 28.869866984347084\n",
            "Episode 900, Loss: 33.182857513427734, Rewards = -59.879250314417575\n",
            "Episode 910, Loss: 243.19442749023438, Rewards = -13.297768876861738\n",
            "Episode 920, Loss: 34.10142517089844, Rewards = 26.34034635337354\n",
            "Episode 930, Loss: 20.184978485107422, Rewards = -73.28305352885361\n",
            "Episode 940, Loss: 26.16072654724121, Rewards = -15.336836338780468\n",
            "Episode 950, Loss: 96.30564880371094, Rewards = -68.27312124455882\n",
            "Episode 960, Loss: 30.530982971191406, Rewards = -113.98701970901861\n",
            "Episode 970, Loss: 31.45640754699707, Rewards = 28.101165403405744\n",
            "Episode 980, Loss: 19.561681747436523, Rewards = -66.37883761830518\n",
            "Episode 990, Loss: 27.305049896240234, Rewards = -97.43407863836961\n",
            "Episode 1000, Loss: 20.422025680541992, Rewards = -132.12673463730502\n",
            "Episode 1010, Loss: 17.489521026611328, Rewards = -109.79358765591084\n",
            "Episode 1020, Loss: 20.653076171875, Rewards = -66.00048955792771\n",
            "Episode 1030, Loss: 88.3542251586914, Rewards = -164.89257493169805\n",
            "Episode 1040, Loss: 14.461280822753906, Rewards = -108.80767811892606\n",
            "Episode 1050, Loss: 89.12763977050781, Rewards = -98.16220368964497\n",
            "Episode 1060, Loss: 18.87417984008789, Rewards = -84.90282841109547\n",
            "Episode 1070, Loss: 21.00191879272461, Rewards = -62.896685508668185\n",
            "Episode 1080, Loss: 20.73576545715332, Rewards = -13.966883818678044\n",
            "Episode 1090, Loss: 23.8737850189209, Rewards = -50.11910427087409\n",
            "Episode 1100, Loss: 13.240621566772461, Rewards = -99.42065555950941\n",
            "Episode 1110, Loss: 27.803125381469727, Rewards = -11.928213212489101\n",
            "Episode 1120, Loss: 15.73202896118164, Rewards = -84.90263693461209\n",
            "Episode 1130, Loss: 20.640644073486328, Rewards = -44.08165612908896\n",
            "Episode 1140, Loss: 25.646690368652344, Rewards = -69.13654960060686\n",
            "Episode 1150, Loss: 20.130130767822266, Rewards = -76.33717825956352\n",
            "Episode 1160, Loss: 25.784767150878906, Rewards = -38.63993934694668\n",
            "Episode 1170, Loss: 20.5167179107666, Rewards = -52.46518194617805\n",
            "Episode 1180, Loss: 12.363761901855469, Rewards = -91.04973292011175\n",
            "Episode 1190, Loss: 18.469032287597656, Rewards = -36.809747306276876\n",
            "Episode 1200, Loss: 15.82922649383545, Rewards = -99.33820945488416\n",
            "Episode 1210, Loss: 19.727310180664062, Rewards = -97.55675047233274\n",
            "Episode 1220, Loss: 27.965951919555664, Rewards = -53.20461920636503\n",
            "Episode 1230, Loss: 19.4481201171875, Rewards = -85.91096895613842\n",
            "Episode 1240, Loss: 28.352035522460938, Rewards = -50.21628383541628\n",
            "Episode 1250, Loss: 23.309633255004883, Rewards = -51.672106185077766\n",
            "Episode 1260, Loss: 13.778300285339355, Rewards = -103.16435797601532\n",
            "Episode 1270, Loss: 17.651411056518555, Rewards = -116.1672723124621\n",
            "Episode 1280, Loss: 8.197214126586914, Rewards = -129.30625066126052\n",
            "Episode 1290, Loss: 18.821346282958984, Rewards = -14.763966196997856\n",
            "Episode 1300, Loss: 10.910216331481934, Rewards = -116.3323630013912\n",
            "Episode 1310, Loss: 11.978687286376953, Rewards = -119.64493389018338\n",
            "Episode 1320, Loss: 21.32944679260254, Rewards = -60.91263503395777\n",
            "Episode 1330, Loss: 9.2652006149292, Rewards = -82.580292143965\n",
            "Episode 1340, Loss: 12.049715995788574, Rewards = -125.01318208238122\n",
            "Episode 1350, Loss: 16.0817813873291, Rewards = -79.67092098477988\n",
            "Episode 1360, Loss: 10.845230102539062, Rewards = -126.8945900991109\n",
            "Episode 1370, Loss: 22.217525482177734, Rewards = -67.06068063290715\n",
            "Episode 1380, Loss: 16.225522994995117, Rewards = -75.88838414880072\n",
            "Episode 1390, Loss: 10.190797805786133, Rewards = -109.1325422056441\n",
            "Episode 1400, Loss: 15.181334495544434, Rewards = -62.27614677481908\n",
            "Episode 1410, Loss: 15.38192081451416, Rewards = -79.5980299163877\n",
            "Episode 1420, Loss: 14.472631454467773, Rewards = -74.67334715407387\n",
            "Episode 1430, Loss: 13.114887237548828, Rewards = -81.93556837727587\n",
            "Episode 1440, Loss: 13.29235553741455, Rewards = -112.62675304528973\n",
            "Episode 1450, Loss: 27.04911994934082, Rewards = -38.06352964724258\n",
            "Episode 1460, Loss: 9.837053298950195, Rewards = -130.52019882061492\n",
            "Episode 1470, Loss: 20.071455001831055, Rewards = -52.5930497959451\n",
            "Episode 1480, Loss: 12.73970890045166, Rewards = -91.06249719080265\n",
            "Episode 1490, Loss: 7.799642086029053, Rewards = -103.69798938390193\n",
            "Episode 1500, Loss: 28.437776565551758, Rewards = -44.93019729358675\n",
            "Episode 1510, Loss: 10.526235580444336, Rewards = -87.7357070880626\n",
            "Episode 1520, Loss: 13.401860237121582, Rewards = -59.98902720477105\n",
            "Episode 1530, Loss: 11.498200416564941, Rewards = -98.67936758395685\n",
            "Episode 1540, Loss: 14.7228364944458, Rewards = -69.8210231887463\n",
            "Episode 1550, Loss: 10.40923023223877, Rewards = -77.95840524102209\n",
            "Episode 1560, Loss: 8.175226211547852, Rewards = -132.22635425673312\n",
            "Episode 1570, Loss: 11.762758255004883, Rewards = -101.51540945905273\n",
            "Episode 1580, Loss: 8.59341812133789, Rewards = -100.01281157761002\n",
            "Episode 1590, Loss: 12.166192054748535, Rewards = -78.39719077442808\n",
            "Episode 1600, Loss: 7.365865230560303, Rewards = -117.99042215325743\n",
            "Episode 1610, Loss: 6.449467182159424, Rewards = -141.88999787544589\n",
            "Episode 1620, Loss: 15.507955551147461, Rewards = -100.85053089676175\n",
            "Episode 1630, Loss: 12.213457107543945, Rewards = -64.47327998055772\n",
            "Episode 1640, Loss: 8.810254096984863, Rewards = -110.53919576221615\n",
            "Episode 1650, Loss: 20.26755714416504, Rewards = -71.84235854520216\n",
            "Episode 1660, Loss: 9.496018409729004, Rewards = -86.24266344614188\n",
            "Episode 1670, Loss: 7.59233283996582, Rewards = -112.09639612514492\n",
            "Episode 1680, Loss: 7.393487930297852, Rewards = -99.06862419659261\n",
            "Episode 1690, Loss: 8.259950637817383, Rewards = -135.7380160480739\n",
            "Episode 1700, Loss: 7.348318576812744, Rewards = -128.57435983808\n",
            "Episode 1710, Loss: 13.50687313079834, Rewards = -62.54088578114019\n",
            "Episode 1720, Loss: 10.467185974121094, Rewards = -89.00418742103959\n",
            "Episode 1730, Loss: 29.78184700012207, Rewards = -64.61853665290649\n",
            "Episode 1740, Loss: 19.9149227142334, Rewards = -55.02434485183007\n",
            "Episode 1750, Loss: 8.582999229431152, Rewards = -129.33519934643923\n",
            "Episode 1760, Loss: 22.726205825805664, Rewards = -54.132659127855334\n",
            "Episode 1770, Loss: 18.35475730895996, Rewards = -98.55991434502391\n",
            "Episode 1780, Loss: 10.104934692382812, Rewards = -113.58690218027877\n",
            "Episode 1790, Loss: 13.109106063842773, Rewards = -71.27597989506495\n",
            "Episode 1800, Loss: 11.836234092712402, Rewards = -80.15718577583819\n",
            "Episode 1810, Loss: 5.929477691650391, Rewards = -133.70911368937837\n",
            "Episode 1820, Loss: 9.523146629333496, Rewards = -68.76921895912889\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "def compute_returns(rewards, gamma=0.9):\n",
        "    returns = []\n",
        "    discounted_reward = 0\n",
        "    for r in reversed(rewards):\n",
        "        discounted_reward = r + gamma * discounted_reward\n",
        "        returns.insert(0, discounted_reward)\n",
        "    return returns\n",
        "\n",
        "def a2c(env_name='LunarLander-v2', num_episodes=10000, max_steps=1000, learning_rate=1e-3, gamma=0.9):\n",
        "    env = gym.make(env_name)\n",
        "    input_dim = env.observation_space.shape[0]\n",
        "    output_dim = env.action_space.n\n",
        "\n",
        "    model = ActorCritic(input_dim, output_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "        sum = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "\n",
        "            state = torch.FloatTensor(state)\n",
        "            policy, value = model(state)\n",
        "            action = torch.multinomial(policy, 1).item()\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            log_prob = torch.log(policy[action])\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "            rewards.append(reward)\n",
        "            sum+=reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        log_probs = torch.stack(log_probs)\n",
        "        values = torch.stack(values)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "\n",
        "        advantage = returns - values\n",
        "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "        critic_loss = advantage.pow(2).mean()\n",
        "\n",
        "        loss = actor_loss + critic_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode}, Loss: {loss.item()}, Rewards = {sum}\")\n",
        "    env.close()\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    a2c()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TltqY-jsm91G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "f75918ac-eb8b-49fd-8b9f-f06eb699db2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Loss: 2979.094970703125, Rewards = -237.82106632534857\n",
            "Episode 10, Loss: 1337.2562255859375, Rewards = -340.77330986182324\n",
            "Episode 20, Loss: 880.9132080078125, Rewards = -150.5912389576531\n",
            "Episode 30, Loss: 662.7376708984375, Rewards = -228.0742497603149\n",
            "Episode 40, Loss: 465.0077819824219, Rewards = -119.56937298375226\n",
            "Episode 50, Loss: 394.7176513671875, Rewards = -103.64194377715857\n",
            "Episode 60, Loss: 520.2841796875, Rewards = -128.41160493037933\n",
            "Episode 70, Loss: 554.1443481445312, Rewards = -129.75239259876426\n",
            "Episode 80, Loss: 664.8717041015625, Rewards = -278.88160411032493\n",
            "Episode 90, Loss: 1806.6627197265625, Rewards = -431.18713562378\n",
            "Episode 100, Loss: 517.2161865234375, Rewards = -69.90422107496009\n",
            "Episode 110, Loss: 2276.50048828125, Rewards = -440.8977235918449\n",
            "Episode 120, Loss: 394.373779296875, Rewards = -125.82979776890664\n",
            "Episode 130, Loss: 370.3176574707031, Rewards = -86.25501834801736\n",
            "Episode 140, Loss: 389.21978759765625, Rewards = -118.43193787873294\n",
            "Episode 150, Loss: 373.1529235839844, Rewards = -108.23401853136846\n",
            "Episode 160, Loss: 352.96685791015625, Rewards = -231.85456156447808\n",
            "Episode 170, Loss: 499.2437744140625, Rewards = -118.7782727760858\n",
            "Episode 180, Loss: 311.2650146484375, Rewards = -170.14549008572692\n",
            "Episode 190, Loss: 428.1463623046875, Rewards = -114.56988327072492\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-294d878085eb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0ma2c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-71-db466bd6648e>\u001b[0m in \u001b[0;36ma2c\u001b[0;34m(env_name, num_episodes, max_steps, learning_rate, gamma)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mtip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mside\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mdispersion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSCALE\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mm_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mtip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mside\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mdispersion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSCALE\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mm_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "final_model= a2c()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import matplotlib.pyplot as plt\n",
        "\n",
        "images=[]\n",
        "sum = 0\n",
        "env = gym.make('LunarLander-v2')\n",
        "state = env.reset()\n",
        "state = torch.FloatTensor(state)\n",
        "policy, value = final_model(state)\n",
        "\n",
        "for steps in range(1000):\n",
        "    #img = env.render(mode='rgb_array')\n",
        "    #images.append(img)\n",
        "\n",
        "    # Find the index of the action with the highest probability\n",
        "    best_action_index = torch.argmax(policy)\n",
        "\n",
        "    # Convert the index tensor to a Python integer\n",
        "    best_action = best_action_index.item()\n",
        "\n",
        "    print(\"Best Action Index:\", best_action_index)\n",
        "    print(\"Best Action:\", best_action)\n",
        "\n",
        "\n",
        "    next_state, reward, done, _ = env.step(best_action)\n",
        "    sum = sum + reward\n",
        "    state = next_state\n",
        "    state = torch.FloatTensor(state)\n",
        "    policy, value = final_model(state)\n",
        "    print(policy, state)\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "    print(sum)"
      ],
      "metadata": {
        "id": "xle0MHUGc0-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import animation\n",
        "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif', fps=30, dpi=72):\n",
        "\n",
        "    #Mess with this to change frame size\n",
        "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
        "\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    anim.save(path + filename, writer='ffmpeg', fps=fps)"
      ],
      "metadata": {
        "id": "aUPpiXGuvWsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_frames_as_gif(images, filename='Lunar_lander_video.mp4', fps=2)"
      ],
      "metadata": {
        "id": "UR2xfOwVJu4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6flB3VS_J0gs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}